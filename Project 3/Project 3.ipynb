{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a89819c",
   "metadata": {},
   "source": [
    "Using any of the three classifiers described in chapter 6 of Natural Language Processing with Python,\n",
    "and any features you can think of, build the best name gender classifier you can.\n",
    "\n",
    "\n",
    "Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the devtest set, and the remaining 6900 words for the training set. Then, starting with the example name gender\n",
    "classifier, make incremental improvements. Use the dev-test set to check your progress. Once you are\n",
    "satisfied with your classifier, check its final performance on the test set.\n",
    "How does the performance on the test set compare to the performance on the dev-test set? Is this what\n",
    "you'd expect?\n",
    "Source: Natural Language Processing with Python, exercise 6.10.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcd67745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IF below gives error \n",
    "# import nltk\n",
    "# nltk.download()\n",
    "# AND follow prompts to downlod corpora\n",
    "from nltk.corpus import names\n",
    "\n",
    "import random\n",
    "names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "          [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "random.Random(512).shuffle(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5367795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.naivebayes import NaiveBayesClassifier\n",
    "import nltk.classify\n",
    "def tryFeatures(f, c = NaiveBayesClassifier, final= False):\n",
    "    train_names = names[1500:]\n",
    "    devtest_names = names[500:1500]\n",
    "    test_names = names[:500]\n",
    "    train_set = [(f(n), g) for (n,g) in train_names]\n",
    "    devtest_set = [(f(n), g) for (n,g) in devtest_names]\n",
    "    test_set = [(f(n), g) for (n,g) in test_names]\n",
    "    classifier = c.train(train_set) \n",
    "    testSet = devtest_set\n",
    "    if(final):\n",
    "        testSet = test_set\n",
    "    return (nltk.classify.accuracy(classifier, train_set),nltk.classify.accuracy(classifier, testSet)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7bfd29df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8609559279950342, 0.823)\n"
     ]
    }
   ],
   "source": [
    "print(tryFeatures(lambda a: {'len':len(a), 'last':a[-2:], 'trim':a[-1:], 'start':a[:3]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f7249d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9581005586592178, 0.726)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.classify \n",
    "tryFeatures(lambda a: {'len':len(a), 'last':a[-2:], 'trim':a[-1:], 'start':a[:3]},nltk.classify.DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "52f237d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.374\n",
      "             2          -0.38761        0.821\n",
      "             3          -0.32736        0.846\n",
      "             4          -0.29859        0.858\n",
      "             5          -0.28080        0.864\n",
      "             6          -0.26820        0.870\n",
      "             7          -0.25854        0.875\n",
      "             8          -0.25076        0.879\n",
      "             9          -0.24428        0.882\n",
      "            10          -0.23876        0.884\n",
      "            11          -0.23397        0.887\n",
      "            12          -0.22977        0.888\n",
      "            13          -0.22603        0.889\n",
      "            14          -0.22268        0.890\n",
      "            15          -0.21965        0.891\n",
      "            16          -0.21690        0.892\n",
      "            17          -0.21438        0.893\n",
      "            18          -0.21207        0.893\n",
      "            19          -0.20993        0.894\n",
      "            20          -0.20795        0.895\n",
      "            21          -0.20611        0.896\n",
      "            22          -0.20439        0.896\n",
      "            23          -0.20278        0.896\n",
      "            24          -0.20127        0.897\n",
      "            25          -0.19985        0.897\n",
      "            26          -0.19851        0.898\n",
      "            27          -0.19725        0.898\n",
      "            28          -0.19605        0.898\n",
      "            29          -0.19492        0.899\n",
      "            30          -0.19384        0.899\n",
      "            31          -0.19282        0.899\n",
      "            32          -0.19184        0.899\n",
      "            33          -0.19091        0.900\n",
      "            34          -0.19002        0.900\n",
      "            35          -0.18916        0.900\n",
      "            36          -0.18835        0.900\n",
      "            37          -0.18757        0.900\n",
      "            38          -0.18682        0.900\n",
      "            39          -0.18610        0.901\n",
      "            40          -0.18541        0.901\n",
      "            41          -0.18474        0.901\n",
      "            42          -0.18410        0.901\n",
      "            43          -0.18348        0.901\n",
      "            44          -0.18288        0.901\n",
      "            45          -0.18231        0.902\n",
      "            46          -0.18175        0.901\n",
      "            47          -0.18121        0.902\n",
      "            48          -0.18070        0.902\n",
      "            49          -0.18019        0.902\n",
      "            50          -0.17971        0.902\n",
      "            51          -0.17923        0.902\n",
      "            52          -0.17878        0.902\n",
      "            53          -0.17833        0.902\n",
      "            54          -0.17790        0.902\n",
      "            55          -0.17748        0.902\n",
      "            56          -0.17708        0.902\n",
      "            57          -0.17668        0.902\n",
      "            58          -0.17630        0.902\n",
      "            59          -0.17593        0.903\n",
      "            60          -0.17556        0.902\n",
      "            61          -0.17521        0.903\n",
      "            62          -0.17487        0.903\n",
      "            63          -0.17453        0.903\n",
      "            64          -0.17420        0.903\n",
      "            65          -0.17389        0.903\n",
      "            66          -0.17357        0.903\n",
      "            67          -0.17327        0.903\n",
      "            68          -0.17298        0.903\n",
      "            69          -0.17269        0.903\n",
      "            70          -0.17241        0.903\n",
      "            71          -0.17213        0.903\n",
      "            72          -0.17186        0.903\n",
      "            73          -0.17160        0.903\n",
      "            74          -0.17134        0.903\n",
      "            75          -0.17109        0.903\n",
      "            76          -0.17085        0.903\n",
      "            77          -0.17060        0.903\n",
      "            78          -0.17037        0.903\n",
      "            79          -0.17014        0.903\n",
      "            80          -0.16991        0.903\n",
      "            81          -0.16969        0.903\n",
      "            82          -0.16948        0.903\n",
      "            83          -0.16927        0.903\n",
      "            84          -0.16906        0.903\n",
      "            85          -0.16886        0.903\n",
      "            86          -0.16866        0.903\n",
      "            87          -0.16846        0.903\n",
      "            88          -0.16827        0.903\n",
      "            89          -0.16808        0.903\n",
      "            90          -0.16790        0.903\n",
      "            91          -0.16772        0.903\n",
      "            92          -0.16754        0.903\n",
      "            93          -0.16737        0.903\n",
      "            94          -0.16719        0.903\n",
      "            95          -0.16703        0.903\n",
      "            96          -0.16686        0.903\n",
      "            97          -0.16670        0.903\n",
      "            98          -0.16654        0.902\n",
      "            99          -0.16638        0.902\n",
      "         Final          -0.16623        0.902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9023898199875854, 0.823)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tryFeatures(lambda a: {'len':len(a), 'last':a[-2:], 'trim':a[-1:], 'start':a[:3]},nltk.classify.MaxentClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b6d533",
   "metadata": {},
   "source": [
    "Looking at the results, a descision tree classifier massively over fits. A naive bayes classifier does surprisingly well. This suggests we use it for the final check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dba816bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8609559279950342, 0.838)\n"
     ]
    }
   ],
   "source": [
    "print(tryFeatures(lambda a: {'len':len(a), 'last':a[-2:], 'trim':a[-1:], 'start':a[:3]}, final =True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d626210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "This does quite well, actually better than devtest. Normally one would expect a regression. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
